---

---

The following notebook is for processing LEHD files

```{r}
library(tidyverse)
library(DBI)
library(dbplyr)
library(connections)
library(httr2)
library(xml2)
library(RPostgres)
library(keyring)
library(duckplyr)
```

```{r}
#| label: connect_to_postgres
# connect to the Postgres database
pg_sdo <- DBI::dbConnect(
  RPostgres::Postgres(),
  dbname = 'dola',
  host = '104.197.26.248',
  port = 5433,
  user = "postgres",
  password = keyring::key_get("pgsdo", "dola")
)

connections::connection_view(pg_sdo)

```

## Mutiple Job Holding Rate

Multiple Job Holdings Rates are derived from the LODES data from the Census Bureau. The multiple job holding rate is derived using the WAC (worker area characteristics) by taking the percentage difference between All Jobs and All Primary Jobs for each area.

First we create a function that can be used for downloading data from the LEHD servers. This function is used for both calculating Multiple Job Holding Rates and for commuting flows.

```{r}
# read in state codes

state_codes <- read_csv(
  "lehd_files/state_codes.txt"
)


```

```{r}

# create function for downloading lehd_data out_of_state flag determines if it downloads the "aux" file, which is for jobs where residence is out of state. LEHD source are options are "od" (origin_destination), "wac" (worker area characteristics), and "rac" (resident area charactersitics). Job types begin with JT00 (for all jobs) with additional JT designations based on designations such as private, private primary, etc
download_lehd_state <- function(
  state_code,
  lehd_source = "od",
  file_type = c("xwalk", "aux", "main"),
  job_type = "JT00"
) {
  # determine where files will go based on file type or state
  folder_ext <- case_when(
    file_type == "xwalk" ~ "xwalk/",
    state_code == "co" ~ "co/",
    .default = "other/"
  )

  # Create a vector of URLs

  lehd_base_url <- ifelse(
    file_type == "xwalk",
    paste0("https://lehd.ces.census.gov/data/lodes/LODES8/", state_code, "/"),
    paste0(
      "https://lehd.ces.census.gov/data/lodes/LODES8/",
      state_code,
      "/",
      lehd_source,
      "/"
    )
  )

  lehd_links <- request(lehd_base_url) |>
    req_perform() |>
    resp_body_html() |>
    xml_find_all("//a")

  lehd_file_list <- xml_attr(lehd_links, "href") |>
    str_subset(file_type) |>
    str_subset(if_else(file_type == "xwalk", file_type, job_type)) |>
    str_subset("csv.gz")

  lehd_urls <- c(paste0(lehd_base_url, lehd_file_list))

  # Set up destination folder
  lehd_dest_folder <- paste0(
    "lehd_files/",
    folder_ext
  )

  dir.create(lehd_dest_folder, showWarnings = FALSE)

  lehd_dest_files <- paste0(lehd_dest_folder, lehd_file_list)

  names(lehd_file_list) <- lehd_file_list |>
    str_extract("\\d{4}") |>
    as.integer()

  # Download files using map
  map2(
    c(
      lehd_urls,
      "https://lehd.ces.census.gov/data/lodes/LODES8/co/co_xwalk.csv.gz"
    ),
    c(
      lehd_dest_files,
      paste0(lehd_dest_folder, "co_xwalk.csv.gz")
    ),
    function(url, dest_file) {
      # check to see if file exists - if it does skip download
      if (file.exists(dest_file)) {
        # cat("File already exists\n")
        return()
      } else {
        tryCatch(
          {
            download.file(url, destfile = dest_file, mode = "wb")
            cat("Downloaded file ", dest_file, "\n")
          },
          error = function(e) {
            cat(
              "Error downloading file ",
              dest_file,
              ":",
              conditionMessage(e),
              "\n"
            )
          }
        )
      }
    }
  )
}
```

```{r}

wac_grid <- (expand_grid(
  state_code = "co",
  lehd_source = "wac",
  file_type = c("S000"),
  job_type = c("JT00", "JT01")
))

wac_results <- pmap(
  wac_grid,
  download_lehd_state,
  .progress = TRUE
)
```

```{r}
states <- state_codes |>
  pull(Code) |>
  unique()
```

```{r}
od_grid <- (expand_grid(
  state_code = str_to_lower(states),
  lehd_source = "od",
  file_type = c("aux"),
  job_type = c("JT00")
)) |>
  bind_rows(c(
    state_code = "co",
    lehd_source = "od",
    file_type = "main",
    job_type = "JT00"
  ))

od_results <- pmap(
  od_grid,
  download_lehd_state,
  .progress = TRUE
)

```

``` {r}
# create function to convert to parquet

# od_missing <- setdiff(
#   list.files(
#     "lehd_files/",
#     pattern = "od.*csv.gz",
#     recursive = TRUE,
#     full.names = TRUE
#   ),
#   read_parquet_duckdb(
#     "lehd_files/lehd_od.parquet"
#   ) |>
#     select(filename) |>
#     distinct() |>
#     pull()
# ) |>
#   read_csv_duckdb(
#     options = list(filename = TRUE)
#   ) |>
#   mutate(
#     w_state = dd$left(w_geocode, 2L),
#     h_state = dd$left(h_geocode, 2L)
#   ) |>
#   filter(h_state == "08" | w_state == "08") |>
#   pull(filename)

# if (length(od_missing) > 0) {
#   lehd_od <- duckplyr::read_csv_duckdb(
#     od_csvs,
#     options = list(filename = TRUE)
#   ) |>
#     mutate(
#       year = as.integer(dd$regexp_extract(filename, "\\d{4}")),
#       w_geocode = dd$left(w_geocode, 5L),
#       w_state = dd$left(w_geocode, 2L),
#       w_cty = dd$substr(w_geocode, 5L),
#       h_geocode = dd$left(h_geocode, 5L),
#       h_state = dd$left(h_geocode, 2L),
#       h_cty = dd$substr(h_geocode, 3L, 5L),
#       same_county = as.integer(w_geocode == h_geocode),
#       .before = 1
#     ) |>
#     filter(h_state == "08" | w_state == "08") |>
#   compute_parquet(
#     "lehd_files/lehd_od.parquet"
#   )
# } else {
#   lehd_od <- read_parquet_duckdb(
#     "lehd_files/lehd_od.parquet"
#   )
# }

```

```{r}
#| label: function_read_files

create_parquet <- function(file_type) {
  csv_pattern <- paste0(file_type, ".*csv.gz")
  parquet_file <- paste0("lehd_files/lehd_", file_type, ".parquet")

  # verify if parquet file exists, if it does then check for missing files to use as
  csv_file_list <- list.files(
    "lehd_files/",
    pattern = csv_pattern,
    recursive = TRUE,
    full.names = TRUE
  )

  if (file.exists(parquet_file)) {
    parquet_file_list <- read_parquet_duckdb(parquet_file) |>
      select(filename) |>
      distinct() |>
      pull()

    csv_file_list <- setdiff(csv_file_list, parquet_file_list)

    if (file_type == "od") {
      csv_file_list <- csv_file_list |>
        read_csv_duckdb(
          options = list(filename = TRUE)
        ) |>
        mutate(
          w_state = dd$left(w_geocode, 2L),
          h_state = dd$left(h_geocode, 2L)
        ) |>
        filter(h_state == "08" | w_state == "08") |>
        pull(filename)
    }
  }

  if (length(csv_file_list) > 0) {
    if (file_type == "od") {
      parquet_tbl <- duckplyr::read_csv_duckdb(
        csv_file_list,
        options = list(
          filename = TRUE
        )
      ) |>
        select(w_geocode, h_geocode, S000, filename) |>
        mutate(
          year = as.integer(dd$regexp_extract(filename, "\\d{4}")),
          w_geocode = dd$left(w_geocode, 5L),
          w_state = dd$left(w_geocode, 2L),
          w_cty = dd$substr(w_geocode, 3L, 5L),
          h_geocode = dd$left(h_geocode, 5L),
          h_state = dd$left(h_geocode, 2L),
          h_cty = dd$substr(h_geocode, 3L, 5L),
          same_county = as.integer(w_geocode == h_geocode),
          .before = 1
        ) |>
        filter(h_state == "08" | w_state == "08")
    } else {
      parquet_tbl <- duckplyr::read_csv_duckdb(
        csv_file_list,
        options = list(
          union_by_name = TRUE,
          filename = TRUE
        )
      )
    }

    compute_parquet(
      parquet_tbl,
      parquet_file
    )

    return(paste0("Processed ", file_type, " files to ", parquet_file))
  } else {
    return(paste0("No new ", file_type, " files to process"))
  }
}

c("wac", "od", "xwalk") |>
  map(create_parquet)


```


```{r}
# read in wac
lehd_mhj <- read_parquet_duckdb("lehd_files/lehd_wac.parquet") |>
  select(w_geocode, C000, filename) |>
  mutate(
    year = as.integer(dd$regexp_extract(filename, "\\d{4}")),
    w_geocode = dd$left(w_geocode, 5L),
    w_state = dd$left(w_geocode, 2L),
    w_cty = dd$substr(w_geocode, 3L, 5L),
    job_type = dd$regexp_extract(filename, "JT.."),
    .after = w_geocode
  ) |>
  summarize(
    total = sum(C000, na.rm = TRUE),
    .by = c(w_geocode, w_state, w_cty, job_type, year)
  ) |>
  pivot_wider(names_from = job_type, values_from = total) |>
  mutate(mult_job_hldg_rate = JT00 / JT01 - 1) |>
  rename(
    all_jobs = JT00,
    primary_jobs = JT01,
    area_fips = w_geocode,
    state_fips = w_state,
    county_fips = w_cty
  ) |>
  arrange(area_fips, year) |>
  collect()
```

```{r}
#| label: write_mhj

# write the lehd_mhj table to Postgres
dbWriteTable(
  pg_sdo,
  name = Id(schema = "econ", table = "lehd_mhj"),
  lehd_mhj,
  overwrite = TRUE,
  row.names = FALSE
)

# write to csv

write_csv(lehd_mhj, "lehd_mhj.csv")
```

# Calculate Commuting Flows

Get commuting flows from LEHD data and calculate net commuters by county. Currently only setup for Colorado - add "*aux*" files instead of main for out of state. Broken into a series of steps.

First download the [LODES OD](https://lehd.ces.census.gov/data/#lodes) data from the Census lehd server.

Next read in the LEHD crosswalk file and create a function to read in and process the LEHD data.

This function reads in the data and processes it to calculate inflow, outflow, and net commuters by county. It also saves the data as an rds file to avoid having to read in the data again.

```{r}
# read in LEHD data
lehd_commute <- read_parquet_duckdb(
  "lehd_files/lehd_od.parquet"
) |>
  summarise(
    total = sum(S000, na.rm = TRUE),
    .by = c(
      year,
      h_cty,
      h_state,
      w_cty,
      w_state,
      same_county
    )
  ) |>
  mutate(
    total_live_work = total * same_county,
    total_commute = total - total_live_work
  ) |>
  collect()

# write the lehd_commute table to Postgres
dbWriteTable(
  pg_sdo,
  name = Id(schema = "econ", table = "lehd_commute"),
  lehd_commute,
  overwrite = TRUE,
  row.names = FALSE
)
```

```{r}
#| label: calculate_lehd_workers
# calculate inflow commuters
lehd_workers <- lehd_commute |>
  filter(w_state == "08") |>
  summarize(
    total_commute_in = sum(total_commute),
    total_live_work = sum(total_live_work),
    .by = c(year, w_state, w_cty)
  ) |>
  arrange(year, w_state, w_cty)

# write the lehd_workers table to Postgres
dbWriteTable(
  pg_sdo,
  name = Id(schema = "econ", table = "lehd_workers"),
  collect(lehd_workers),
  overwrite = TRUE,
  row.names = FALSE
)

lehd_workers <- tbl(lehd_con, "lehd_workers")

```

```{r}
#| label: calculate_lehd_residents

# calculate outflow commuters
lehd_residents <- lehd_commute |>
  filter(h_state == "08") |>
  summarize(
    total_commute_out = -sum(total_commute),
    .by = c(year, h_state, h_cty)
  ) |>
  arrange(year, h_state, h_cty)

# wrote the lehd_residents table to Postgres
dbWriteTable(
  pg_sdo,
  name = Id(schema = "econ", table = "lehd_residents"),
  lehd_residents,
  overwrite = TRUE,
  row.names = FALSE
)


```

```{r}
# calculate net commuter
lehd_net <- lehd_workers |>
  rename(cty = w_cty) |>
  full_join(
    lehd_residents |>
      rename(cty = h_cty)
  ) |>
  mutate(
    across(starts_with("total"), ~ replace_na(., 0)),
    year = year,
    net_commute = total_commute_in + total_commute_out
  ) |>
  select(
    year,
    county_fips = cty,
    total_live_work,
    total_commute_in,
    total_commute_out,
    net_commute
  ) |>
  arrange(year, county_fips)

# write the lehd_net table to Postgres
dbWriteTable(
  pg_sdo,
  name = Id(schema = "econ", table = "lehd_net"),
  lehd_net,
  overwrite = TRUE,
  row.names = FALSE
)

write_csv(lehd_net, "lehd_commute.csv")
```

